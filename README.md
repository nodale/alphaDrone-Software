# alphaDrone-Software

### Overview
This repository, specifically the subdirectory `ai` contains experiments with the ros-free versions of nvblox and librealsense2. 
The goal was to create a more lightweight version not based on ROS. However for simple plug-and-play functionality, ROS is still the best choice.
The `ai` folder is mainly for educational purposes and experiments.

### nvblox and librealsense2
Nvblox is a real-time 3D reconstruction library published by NVIDIA. It's central entity is a Mapper class that takes in depth images, camera poses and color images. For the sake of our experiments, we only tried to extract a mesh from it, but it supports also more advanced features. The goal was rather to understand the interface and how it may work with librealsense2 in a ros-free environment. The main takeways here is, that before using the Mapper, one has to convert the frames emitted by librealsense2 to the respective data structures used by nvblox. Additional the camera paramters have to be extracted from librealsense2.
librealsense2 is a library provided by Intel to interface with their realsense cameras. Its API emits indiviual frames which can be configured to be of different types (depth, color, infrared, etc.). To get a set of frames for all selected channels, one can use the frameset which contains the actual data. The main problem is to retrieve the camera pose. For the ros-based workflow, isaac offers the cuVSLAM-node which can be used for this purpose. However for the ros-free workflow, this has to be done manually. To this end, we experimented a little with the gyro and accel data provided by the IMU of the d435i camera. To get more accurate data here it may be useful to use an (Extended) Kalman Filter. We just implemented a simple method that periodically retrieves the data and increments it. This is relatively straightforward for the gyro data, as its coordinate systems is fixed. The coordinate system of the accel data however is changing with the orientation, so the gravitational acts on a different axis depending on the orientation. In any case the coordinate system is fixed wrt to the camera, so if the camera is rotating the coordinate system is rotating as well. As such the accel has to be projected back to a reference to be able to subtract the gravitational component and to integrate it. To get this reference, the logic for getting the movement from the accel data should be calibrated once in the beginning in a setting where only the gravitational component acts on the camera. A Software Solution for this issue may be openVINS, but it comes with its own set of challenges. In general it may be easier to use the simple integrated method before trying openVINS. 
